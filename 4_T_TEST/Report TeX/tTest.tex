%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Programming/Coding Assignment
% LaTeX Template
%
% This template has been downloaded from:
% http://www.latextemplates.com
%
% Original author:
% Ted Pavlic (http://www.tedpavlic.com)
%
% Note:
% The \lipsum[#] commands throughout this template generate dummy text
% to fill the template out. These commands should all be removed when 
% writing assignment content.
%
% This template uses a Perl script as an example snippet of code, most other
% languages are also usable. Configure them in the "CODE INCLUSION 
% CONFIGURATION" section.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[a4paper]{article}

\usepackage{fancyhdr} % Required for custom headers
\usepackage{lastpage} % Required to determine the last page for the footer
\usepackage{extramarks} % Required for headers and footers
\usepackage[usenames,dvipsnames]{color} % Required for custom colors
\usepackage{graphicx} % Required to insert images
\usepackage{listings} % Required for insertion of code
\usepackage{courier} % Required for the courier font
\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template
\usepackage{multirow}
\usepackage{tabu}
\usepackage{float}
\usepackage{pbox}
\usepackage{subcaption}
\usepackage{algpseudocode}

\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Matlab,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true
  tabsize=3
}

% Margins
\topmargin=-0.45in
\evensidemargin=0in
\oddsidemargin=0in
\textwidth=6.5in
\textheight=9.5in
\headsep=0.25in

\linespread{1.1} % Line spacing

% Set up the header and footer
\pagestyle{fancy}
\lhead{\hmwkAuthorName} % Top left header
\chead{\hmwkShortTitle} % Top center head
\rhead{\firstxmark} % Top right header
\lfoot{\lastxmark} % Bottom left footer
\cfoot{} % Bottom center footer
\rfoot{Page\ \thepage\ of\ \protect\pageref{LastPage}} % Bottom right footer
\renewcommand\headrulewidth{0.4pt} % Size of the header rule
\renewcommand\footrulewidth{0.4pt} % Size of the footer rule

\setlength\parindent{0pt} % Removes all indentation from paragraphs

%----------------------------------------------------------------------------------------
%	CODE INCLUSION CONFIGURATION
%----------------------------------------------------------------------------------------

\definecolor{MyDarkGreen}{rgb}{0.0,0.4,0.0} % This is the color used for comments
\lstloadlanguages{Matlab} % Load Perl syntax for listings, for a list of other languages supported see: ftp://ftp.tex.ac.uk/tex-archive/macros/latex/contrib/listings/listings.pdf
\lstset{language=Matlab, % Use Perl in this example
        frame=single, % Single frame around code
        basicstyle=\small\ttfamily, % Use small true type font
        keywordstyle=[1]\color{Blue}\bf, % Perl functions bold and blue
        keywordstyle=[2]\color{Purple}, % Perl function arguments purple
        keywordstyle=[3]\color{Blue}\underbar, % Custom functions underlined and blue
        identifierstyle=, % Nothing special about identifiers                                         
        commentstyle=\usefont{T1}{pcr}{m}{sl}\color{MyDarkGreen}\small, % Comments small dark green courier font
        stringstyle=\color{Purple}, % Strings are purple
        showstringspaces=false, % Don't put marks in string spaces
        tabsize=5, % 5 spaces per tab
        %
        % Put standard Perl functions not included in the default language here
        morekeywords={rand},
        %
        % Put Perl function parameters here
        morekeywords=[2]{on, off, interp},
        %
        % Put user defined functions here
        morekeywords=[3]{test},
       	%
        morecomment=[l][\color{Blue}]{...}, % Line continuation (...) like blue comment
        numbers=left, % Line numbers on left
        firstnumber=1, % Line numbers start with line 1
        numberstyle=\tiny\color{Blue}, % Line numbers are blue and small
        stepnumber=5 % Line numbers go in steps of 5
}

% Creates a new command to include a perl script, the first parameter is the filename of the script (without .pl), the second parameter is the caption
\newcommand{\matlabscript}[2]{
\begin{itemize}
\item[]\lstinputlisting[caption=#2,label=#1]{#1.m}
\end{itemize}
}

%----------------------------------------------------------------------------------------
%	DOCUMENT STRUCTURE COMMANDS
%	Skip this unless you know what you're doing
%----------------------------------------------------------------------------------------

% Header and footer for when a page split occurs within a problem environment
\newcommand{\enterProblemHeader}[1]{
\nobreak\extramarks{#1}{#1 continued on next page\ldots}\nobreak
\nobreak\extramarks{#1 (continued)}{#1 continued on next page\ldots}\nobreak
}

% Header and footer for when a page split occurs between problem environments
\newcommand{\exitProblemHeader}[1]{
\nobreak\extramarks{#1 (continued)}{#1 continued on next page\ldots}\nobreak
\nobreak\extramarks{#1}{}\nobreak
}

\setcounter{secnumdepth}{0} % Removes default section numbers
\newcounter{homeworkProblemCounter} % Creates a counter to keep track of the number of problems

\newcommand{\homeworkProblemName}{}
\newenvironment{homeworkProblem}[1][Problem \arabic{homeworkProblemCounter}]{ % Makes a new environment called homeworkProblem which takes 1 argument (custom name) but the default is "Problem #"
\stepcounter{homeworkProblemCounter} % Increase counter for number of problems
\renewcommand{\homeworkProblemName}{#1} % Assign \homeworkProblemName the name of the problem
\section{\homeworkProblemName} % Make a section in the document with the custom problem count
\enterProblemHeader{\homeworkProblemName} % Header and footer within the environment
}{
\exitProblemHeader{\homeworkProblemName} % Header and footer after the environment
}

\newcommand{\problemAnswer}[1]{ % Defines the problem answer command with the content as the only argument
\noindent\framebox[\columnwidth][c]{\begin{minipage}{0.98\columnwidth}#1\end{minipage}} % Makes the box around the problem answer and puts the content inside
}

\newcommand{\homeworkSectionName}{}
\newenvironment{homeworkSection}[1]{ % New environment for sections within homework problems, takes 1 argument - the name of the section
\renewcommand{\homeworkSectionName}{#1} % Assign \homeworkSectionName to the name of the section from the environment argument
\subsection{\homeworkSectionName} % Make a subsection with the custom name of the subsection
\enterProblemHeader{\homeworkProblemName\ [\homeworkSectionName]} % Header and footer within the environment
}{
\enterProblemHeader{\homeworkProblemName} % Header and footer after the environment
}

%----------------------------------------------------------------------------------------
%	NAME AND CLASS SECTION
%----------------------------------------------------------------------------------------

\newcommand{\hmwkTitle}{CO395 Machine Learning\\CBC \#4\\t-test} % Assignment title
\newcommand{\hmwkShortTitle}{CBC \#4 - t-test}
\newcommand{\hmwkDueDate}{Monday,\ November\ 4,\ 2013} % Due date
\newcommand{\hmwkAuthorName}{Group 1} % Your name

%----------------------------------------------------------------------------------------
%	TITLE PAGE
%----------------------------------------------------------------------------------------

\title{
\vspace{2in}
\textmd{\textbf{\hmwkTitle}}\\
%\normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate}\\
%\vspace{0.1in}\large{\textit{\hmwkClassInstructor\ \hmwkClassTime}}
\vspace{3in}
\textbf{Group 1}\\
Yong Wen Chua, \texttt{ywc110}\\
Thomas Morrison, \texttt{tm1810}\\
Marcin Baginski, \texttt{mgb10}\\
Marcin Kadziela, \texttt{mk4910}
}

%\author{\textbf{\hmwkAuthorName}}

\date{} % Insert date here if you want it to appear below your name

%----------------------------------------------------------------------------------------

\begin{document}

\maketitle

%----------------------------------------------------------------------------------------
%	TABLE OF CONTENTS
%----------------------------------------------------------------------------------------

%\setcounter{tocdepth}{1} % Uncomment this line if you don't want subsections listed in the ToC

\newpage
\tableofcontents
\newpage

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

\section{Results of the t-test}

\subsection{Clean dataset}

\begin{table}[H]
\center
\begin{tabular}{c|c|c|c|}
\cline{2-4}
 & DT vs. ANN & DT vs. CBR & ANN vs. CBR \\ \hline
\multicolumn{1}{ |c| }{Emotion 1} & 3.0498 & 8.9429 & 3.9477 \\ \hline
\multicolumn{1}{ |c| }{Emotion 2} & 4.2663 & 3.9580 & 1.0657 \\ \hline
\multicolumn{1}{ |c| }{Emotion 3} & 1.0668 & 6.0045 & 2.2500 \\ \hline
\multicolumn{1}{ |c| }{Emotion 4} & 4.6751 & 6.2453 & -1.2377 \\ \hline
\multicolumn{1}{ |c| }{Emotion 5} & 3.8522 & 6.9003 & 5.5578 \\ \hline
\multicolumn{1}{ |c| }{Emotion 6} & 2.9902 & 3.5254 & 0.5006 \\ \hline
\end{tabular}
\caption{t-values for every emotion and algorithm on the \emph{clean} dataset}
\label{tValuesClean}
\end{table}

\begin{table}[H]
\center
\begin{tabular}{c|c|c|c|}
\cline{2-4}
 & DT vs. ANN & DT vs. CBR & ANN vs. CBR \\ \hline
\multicolumn{1}{ |c| }{Emotion 1} & different & different & different \\ \hline
\multicolumn{1}{ |c| }{Emotion 2} & different & different & similar \\ \hline
\multicolumn{1}{ |c| }{Emotion 3} & similar & different & similar \\ \hline
\multicolumn{1}{ |c| }{Emotion 4} & different & different & similar \\ \hline
\multicolumn{1}{ |c| }{Emotion 5} & different & different & different \\ \hline
\multicolumn{1}{ |c| }{Emotion 6} & different & different & similar \\ \hline
\end{tabular}
\caption{t-values for every emotion and algorithm on the \emph{clean} dataset}
\label{tValuesInterpretationClean}
\end{table}

\subsection{Noisy dataset}

\begin{table}[H]
\center
\begin{tabular}{c|c|c|c|}
\cline{2-4}
 & DT vs. ANN & DT vs. CBR & ANN vs. CBR \\ \hline
\multicolumn{1}{ |c| }{Emotion 1} & 7.7947 & 8.4406 & 1.0476 \\ \hline
\multicolumn{1}{ |c| }{Emotion 2} & 6.2755 & 7.9588 & 2.2119 \\ \hline
\multicolumn{1}{ |c| }{Emotion 3} & 2.0966 & 1.9214 & 0.2191 \\ \hline
\multicolumn{1}{ |c| }{Emotion 4} & 2.5572 & 5.3489 & 0.5100 \\ \hline
\multicolumn{1}{ |c| }{Emotion 5} & 1.0530 & 1.8700 & 1.3814 \\ \hline
\multicolumn{1}{ |c| }{Emotion 6} & 3.9404 & 3.0950 & -0.7212 \\ \hline
\end{tabular}
\caption{t-values for every emotion and algorithm on the \emph{noisy} dataset}
\label{tValuesNoisy}
\end{table}

\begin{table}[H]
\center
\begin{tabular}{c|c|c|c|}
\cline{2-4}
 & DT vs. ANN & DT vs. CBR & ANN vs. CBR \\ \hline
\multicolumn{1}{ |c| }{Emotion 1} & different & different & similar \\ \hline
\multicolumn{1}{ |c| }{Emotion 2} & different & different & similar \\ \hline
\multicolumn{1}{ |c| }{Emotion 3} & similar & similar & similar \\ \hline
\multicolumn{1}{ |c| }{Emotion 4} & similar & different & similar \\ \hline
\multicolumn{1}{ |c| }{Emotion 5} & similar & similar & similar \\ \hline
\multicolumn{1}{ |c| }{Emotion 6} & different & different & similar \\ \hline
\end{tabular}
\caption{Interpretation of the t-values for every algorithm for the \emph{noisy} dataset}
\label{tValuesInterpretationNoisy}
\end{table}

\clearpage

%----------------------------------------------------------------------------------------
%	SECTION 2
%----------------------------------------------------------------------------------------

\section{Questions}

\subsection{Performance of the algorithms}

For the \emph{clean} dataset, it looks that the CBR algorithm performed clearly better than the Decision Trees. The error rates were statistically different for each emotion and further, manual examination of them shows, that they were almost always smaller in the CBR. It seems that the Artificial Neural Networks perform statistically better than the Decision Trees for most emotions (except emotion 3). It is difficult to assess the performance of the ANN vs. CBR since the t-test returned mixed results. \medskip

In case of the \emph{noisy} dataset, all three algorithms perform similarly and it is difficult to assess whether any of them has performed significantly better. This is particularly the case in ANN vs. CBR since according to the t-test the distributions of errors for both algorithms across all the emotions are statistically similar. \medskip

Having said that, it cannot be claimed that any of the algorithms is clearly a better learning technique than the others. The performance of the algorithms is heavily dependent on the data which needs to be classified and the implementation (especially in case of the CBR). For example, let's assume that we have to classify a vector of 5 features, $[a,b,c,d,e]$ to a binary class. Additionally, let's assume that the first feature $a$ is the most decisive one and carries the most information for the classification. The Decision Trees would actually perform very well on this particular problem, since during training they rely on the information gain of each attribute. Especially if we pruned the tree after the first iteration of the learning algorithm, it might achieve superb performance. On the other hand, a simple implementation of the CBR would try to compare the entire feature vector, even though the variables $[b,c,d,e]$ might actually carry no information whatsoever.

\subsection{Adjustment of the significance level}

Since we have 3 algorithms and we want to compare each with every other, we need $\frac{3 \times 2}{2} = 3$ multiple comparisons. Our initially chose significance level was $\alpha = 0.05$ which, after applying the Bonferroni correction, is equal to $\alpha = \frac{0.05}{3} \approx 0.02$. The t-value for 9 degrees of freedom and $\alpha = 0.02$, which we used to determine whether the samples are statistically different, is $t = 2.821$.

\subsection{Type of the t-test}

In each fold for each algorithm the test set consisted of the same examples. For this reasion, we used the paired t-test because the values of the error rate which we are comparing are clearly not independent.

\subsection{Classification error vs. $F_1$ measure}

F1 measure is based only on the true positives, false positives and false negatives, while the error rate also considers the true negatives. For this reason, the error rate is a more comprehensive measure and hence better suited for the t-test.

\subsection{Trade-off between the number of folds and examples per fold}

We have a fixed number of examples (1004 for the \emph{clean} dataset and 1001 for the \emph{noisy} dataset) which we should split into \texttt{n} folds. Increasing the number of examples in each fold also increases the accuracy/resolution of the confusion matrix. In other words, if we assume that the confusion matrix follows some probability distribution (and so does the error rate for this matrix), then increasing the number of examples for each fold is like drawing more samples from this probability distribution. Obviously, if we have more samples from a given probability distribution, then we can compute a better approximation of its probability density function. To sum up, having more test examples per fold allows us to generate a better approximation of the performance of the algorithm for the unseen data. \medskip

The disadvantage of increasing number of samples in each fold is that we are decreasing the number of times we can compute the error rate which we use in the t-test. By having the smaller number of samples for the t-test we are essentially decreasing the confidence that it will return meaningful results. For example, if we used the t-test to assess only four samples drawn from Normal distribution with the exact same mean and variance, the t-test might often fail and claim that they come from distributions with two different means. On the other hand, comparing 10000 samples will reduce the probability of such error almost to zero. The same principle can be applied to our case of trying to determine whether the error rates for two different algorithms are statistically different.

\subsection{Additional emotions}

In the answer to this question we are assuming that the new emotions do not change the classifications assigned to the already existing data. \medskip

If we wanted to add new emotions to the dataset, the Case Based Reasoning algorithm would require the fewest changes. We would simply add the new examples to the existing Case Base which would be the only required change. \medskip

The Decision Trees and Neural Networks however would require a complete re-training. We would need to partition the new set of examples together with the old ones into training and validation set and then run the training algorithm. For the Decision Trees and the Neural Networks, the existing trees and networks would have to be completely discarded and their number would also change (especially if we used \texttt{n} single-output ANNs). Also, for the ANN, we would additionally need to optimise the entire set of parameters, which is the a very computationally expensive task.

\clearpage

\section{Appendix 1 - error rates on the clean dataset}

\subsection{Decision Trees}

\begin{table}[H]
\center
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|}
\cline{2-11}
 & Fold 1 & Fold 2 & Fold 3 & Fold 4 & Fold 5 & Fold 6 & Fold 7 & Fold 8 & Fold 9 & Fold 10 \\ \hline
\multicolumn{1}{ |c| }{Emotion 1} & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% \\ \hline
\multicolumn{1}{ |c| }{Emotion 2} & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% \\ \hline
\multicolumn{1}{ |c| }{Emotion 3} & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% \\ \hline
\multicolumn{1}{ |c| }{Emotion 4} & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% \\ \hline
\multicolumn{1}{ |c| }{Emotion 5} & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% \\ \hline
\multicolumn{1}{ |c| }{Emotion 6} & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% \\ \hline
\end{tabular}
\caption{Error rates for each fold and each emotion returned by the Decision Trees algorithm on the \emph{clean} dataset}
\label{errorsCleanDT}
\end{table}

\subsection{Artificial Neural Networks}

\begin{table}[H]
\center
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|}
\cline{2-11}
 & Fold 1 & Fold 2 & Fold 3 & Fold 4 & Fold 5 & Fold 6 & Fold 7 & Fold 8 & Fold 9 & Fold 10 \\ \hline
\multicolumn{1}{ |c| }{Emotion 1} & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% \\ \hline
\multicolumn{1}{ |c| }{Emotion 2} & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% \\ \hline
\multicolumn{1}{ |c| }{Emotion 3} & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% \\ \hline
\multicolumn{1}{ |c| }{Emotion 4} & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% \\ \hline
\multicolumn{1}{ |c| }{Emotion 5} & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% \\ \hline
\multicolumn{1}{ |c| }{Emotion 6} & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% \\ \hline
\end{tabular}
\caption{Error rates for each fold and each emotion returned by the ANN algorithm on the \emph{clean} dataset}
\label{errorsCleanANN}
\end{table}

\subsection{Case Based Reasoning}

\begin{table}[H]
\center
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|}
\cline{2-11}
 & Fold 1 & Fold 2 & Fold 3 & Fold 4 & Fold 5 & Fold 6 & Fold 7 & Fold 8 & Fold 9 & Fold 10 \\ \hline
\multicolumn{1}{ |c| }{Emotion 1} & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% \\ \hline
\multicolumn{1}{ |c| }{Emotion 2} & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% \\ \hline
\multicolumn{1}{ |c| }{Emotion 3} & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% \\ \hline
\multicolumn{1}{ |c| }{Emotion 4} & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% \\ \hline
\multicolumn{1}{ |c| }{Emotion 5} & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% \\ \hline
\multicolumn{1}{ |c| }{Emotion 6} & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% \\ \hline
\end{tabular}
\caption{Error rates for each fold and each emotion returned by the CBR algorithm on the \emph{clean} dataset}
\label{errorsCleanCBR}
\end{table}

\clearpage

\section{Appendix 2 - error rates of the noisy dataset}

\subsection{Decision Trees}

\begin{table}[H]
\center
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|}
\cline{2-11}
 & Fold 1 & Fold 2 & Fold 3 & Fold 4 & Fold 5 & Fold 6 & Fold 7 & Fold 8 & Fold 9 & Fold 10 \\ \hline
\multicolumn{1}{ |c| }{Emotion 1} & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% \\ \hline
\multicolumn{1}{ |c| }{Emotion 2} & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% \\ \hline
\multicolumn{1}{ |c| }{Emotion 3} & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% \\ \hline
\multicolumn{1}{ |c| }{Emotion 4} & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% \\ \hline
\multicolumn{1}{ |c| }{Emotion 5} & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% \\ \hline
\multicolumn{1}{ |c| }{Emotion 6} & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% \\ \hline
\end{tabular}
\caption{Error rates for each fold and each emotion returned by the Decision Trees algorithm on the \emph{noisy} dataset}
\label{errorsNoisyDT}
\end{table}

\subsection{Artificial Neural Networks}

\begin{table}[H]
\center
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|}
\cline{2-11}
 & Fold 1 & Fold 2 & Fold 3 & Fold 4 & Fold 5 & Fold 6 & Fold 7 & Fold 8 & Fold 9 & Fold 10 \\ \hline
\multicolumn{1}{ |c| }{Emotion 1} & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% \\ \hline
\multicolumn{1}{ |c| }{Emotion 2} & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% \\ \hline
\multicolumn{1}{ |c| }{Emotion 3} & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% \\ \hline
\multicolumn{1}{ |c| }{Emotion 4} & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% \\ \hline
\multicolumn{1}{ |c| }{Emotion 5} & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% \\ \hline
\multicolumn{1}{ |c| }{Emotion 6} & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% \\ \hline
\end{tabular}
\caption{Error rates for each fold and each emotion returned by the ANN algorithm on the \emph{noisy} dataset}
\label{errorsNoisyANN}
\end{table}

\subsection{Case Based Reasoning}

\begin{table}[H]
\center
\begin{tabular}{c|c|c|c|c|c|c|c|c|c|c|}
\cline{2-11}
 & Fold 1 & Fold 2 & Fold 3 & Fold 4 & Fold 5 & Fold 6 & Fold 7 & Fold 8 & Fold 9 & Fold 10 \\ \hline
\multicolumn{1}{ |c| }{Emotion 1} & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% \\ \hline
\multicolumn{1}{ |c| }{Emotion 2} & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% \\ \hline
\multicolumn{1}{ |c| }{Emotion 3} & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% \\ \hline
\multicolumn{1}{ |c| }{Emotion 4} & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% \\ \hline
\multicolumn{1}{ |c| }{Emotion 5} & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% \\ \hline
\multicolumn{1}{ |c| }{Emotion 6} & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% & 0\% \\ \hline
\end{tabular}
\caption{Error rates for each fold and each emotion returned by the CBR algorithm on the \emph{noisy} dataset}
\label{errorsNoisyCBR}
\end{table}

\clearpage

%----------------------------------------------------------------------------------------

\end{document}